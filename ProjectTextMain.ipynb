{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My project for this course is looking at breast cancer data from an ICAR breast cancer history competition using a densenet CNN developed by my colleague in my lab Haben Berhane and optimizing it further.\n",
    "\n",
    "This project looks at the different methods that tensorflow utilizes that builds off the basics we have learned in the course.  These examples include:\n",
    "\n",
    "Momentum based learning: Nesterov momentum, ADAM, ADAGRAD\n",
    "\n",
    "Step based learning: .1, .01, and .001 steps were investigated\n",
    "\n",
    "Regularization: .1, .01, and .001 regularization was looked at\n",
    "\n",
    "Batch Learning: full and half batch learning was investigated.\n",
    "\n",
    "The explaination of these will be discused below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Momentum\n",
    "\n",
    "In class we talked about a standard momentum function which modifies the direction of the gradient descent by also weighing in the previous direction.  That is\n",
    "\n",
    "$$w^k = w^{k-1} + \\alpha d^{k-1} $$\n",
    "$$ d^{k-1} = \\beta d^{k-2} -  (1 - \\beta) \\nabla g(w)$$\n",
    "\n",
    "This functioned as to fix the zig-zag nature of gradient descent, especially in narrow quadratic functions.  Nesterov Momenum also does this concept but in a different fashion.\n",
    "\n",
    "Nesterov Momentum instead is:\n",
    "$$w^k = w^{k-1} + d^{k-1} $$\n",
    "$$ d^{k-1} = \\beta d^{k-2} -  \\alpha \\nabla g(w+ \\beta*d^{k-2}) $$\n",
    "\n",
    "Nesterov momentum tends to converge faster than classical momentum for complex cost functions.  By taking the first \"big\" jump and then correcting it, we can travel further in the cost function space than regular momentum, but also ameliorate the zig-zagging by correcting.\n",
    "\n",
    "[Reference]: \n",
    "\n",
    "https://stats.stackexchange.com/questions/179915/whats-the-difference-between-momentum-based-gradient-descent-and-nesterovs-acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.stack.imgur.com/wBIpz.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://i.stack.imgur.com/wBIpz.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAM Moment\n",
    "\n",
    "ADAM is another optimization for momentum. Formally, it functions as:\n",
    "\n",
    "$$w^k = w^{k-1} - \\frac{\\alpha}{\\sqrt{v^{k-1}} + \\epsilon} m^{k-1}  $$\n",
    "\n",
    "$$m^{k} = \\frac{1}{1-\\beta_1^k} (\\beta_1 m^{k-1} + (1- \\beta_1) g(w^k) $$\n",
    "\n",
    "$$v^k = \\frac{1}{1-\\beta_2^k} (\\beta_2 v^{k-1} + (1-\\beta_2) g(w^k)^2 $$\n",
    "\n",
    "Where the first term, represents an exponential decay of that tends to 1 as iterations continue.  This is due to the inital condition being something that can be converged to, so early on, you want to weight the input from $m$ and $v$ less.  This momentum optimizer is really looking for flat minima error surfaces.  It also combines previous gradient and a previous gradient squared information, augmenting classical momentum (by m and v respectively).\n",
    "\n",
    "http://ruder.io/optimizing-gradient-descent/index.html#adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAGRAD Momentum\n",
    "\n",
    "ADAGRAD is the 3rd momenumt optimizer I looked at.  It changes the learning parameter based upon feature density and thus works well with sparse data (which  may be useful as these images tend to be fairly uniform with low contrast and could be sparse in that regard).\n",
    "\n",
    "It changes our work as follows:\n",
    "\n",
    "$$w^k_i = w^{k-1}_i - \\frac{\\alpha}{\\sqrt{G^{k-1}_{ii} + \\epsilon}} g(w^{k-1}_i)$$\n",
    "\n",
    "So here it is stepping through each feature a different way, chiefly based on the squared gradients of previous time steps summed up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project:\n",
    "\n",
    "For this project, I used the above momentum optimizers with different learning rates on full batch data to generate 2 epochs of learning with the loss being used to determine how well the neural network was performing.\n",
    "\n",
    "I then performed a half and single batch of the best learning parameter with each momentum for 2 epochs. I also tested different regularization constants for the best learning parameter choice with full batch as well.\n",
    "\n",
    "2 epochs were used as training could take 1 to 1.5 hours on my laptop for each session, so even though I started the project early, I couldn't have my laptop out of comission to run the code that often, so I made the practical decision to run many different optimization schemes over a short duration to get a flavor for how they performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Coniditions\n",
    "\n",
    "Here were the initial conditions for each momentum type below:\n",
    "\n",
    "$\\textbf{Nesterov}$:\n",
    "\n",
    "init_learning_rate = 0.1\n",
    "\n",
    "dropout_rate = 0.2\n",
    "\n",
    "nesterov_momentum = 0.9\n",
    "\n",
    "weight_decay = 10\n",
    "\n",
    "exp_decay_steps = 6000\n",
    "\n",
    "exp_decay_rate = 1\n",
    "\n",
    "regularization = .0001\n",
    "\n",
    "\n",
    "$\\textbf{ADAM}$:\n",
    "\n",
    "same as above but\n",
    "\n",
    " beta1=0.9, \n",
    " \n",
    " beta2=0.999, \n",
    " \n",
    " epsilon=1e-8\n",
    "\n",
    "\n",
    "$\\textbf{ADA}$:\n",
    "\n",
    "same as above but\n",
    "\n",
    "initial_accumulator_value=0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Parameter\n",
    "\n",
    "For 1.0 learning the momentum optmizers performed as: \n",
    "\n",
    "ADAGRAD: 29.09683 , 33.38682 , 33.34616 , 33.33678\n",
    "\n",
    "ADAM: 20.70812 , 48.04075 , 134.737 , 76.29205\n",
    "\n",
    "Nesterov: 101.6043 , 1.5E+10 , nan , nan\n",
    "\n",
    "For .01 learning:\n",
    "\n",
    "ADAGRAD: 41.20542 , 35.09968 , 36.40806 , 33.60392\n",
    "\n",
    "ADAM: 13.20862 , 26.06066 , 22.15719 , 23.21645\n",
    "\n",
    "Nesterov: 38.48826 , 71.19455 , 52.33072 , $\\textbf{25.16515}$\n",
    "\n",
    "For .001 learning:\n",
    "\n",
    "ADAGRAD: 35.0094 , 33.44678 , 35.32433 , $\\textbf{31.89558}$\n",
    "\n",
    "ADAM: 19.09145 , 16.81667 , 13.19629 , $\\textbf{18.54706}$\n",
    "\n",
    "Nesterov: 38.1938, 15.957069, 21.710157, 26.298061\n",
    "\n",
    "So the bolded values indicate the lowest loss and were used for the learning prameter for determing regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Using the best learning rate, I then moved onto determing the best regularization\n",
    "\n",
    ".0001 regularization:\n",
    "\n",
    "ADAGRAD: 35.0094 , 33.44678 , 35.32433 , 31.89558\n",
    "\n",
    "ADAM: 19.09145 , 16.81667 , 13.19629 , 18.54706\n",
    " \n",
    "Nesterov: 38.48826 , 71.19455 , 52.33072 , 25.16515\n",
    "\n",
    ".001:\n",
    "\n",
    "ADAGRAD: 38.40235 , 24.26172 , 24.5208 , 22.04238\n",
    "\n",
    "ADAM: 50.26275 , 24.89122 , 19.50465 ,20.66568\n",
    "\n",
    "Nesterov: 33.01615 , 22.5851 , 10.08508 , 27.59055\n",
    "\n",
    ".01:\n",
    "\n",
    "ADAGRAD: 32.16628 , 21.32779 , 18.086 , $\\textbf{15.82132}$\n",
    "\n",
    "ADAM: 58.83173 , 19.95081 , 23.7641 , 17.11846\n",
    "\n",
    "Nesterov: 33.28561 , 30.74485 , 24.63548 ,  $\\textbf{16.47052}$\n",
    "\n",
    ".1:\n",
    "\n",
    "ADAGRAD: 35.02432 , 21.66205 , 25.99742 , 18.49324\n",
    "\n",
    "ADAM: 45.23226 , 13.65605 , 22.18794 , $\\textbf{15.33831}$\n",
    "\n",
    "Nesterov: 37.52027 , 20.29804 , 24.76661 , 29.77377\n",
    "\n",
    "Again, the bolded values are the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Learning\n",
    "\n",
    "So with these best learning paramters and regularizations, I then compared full 48 batch to half 24 batch.\n",
    "\n",
    "Full batch:\n",
    "\n",
    "ADAGRAD: 32.16628 , 21.32779 , 18.086 , $\\textbf{15.82132}$\n",
    "\n",
    "ADAM: 45.23226 , 13.65605 , 22.18794 , $\\textbf{15.33831}$\n",
    "\n",
    "Nesterov: 33.28561 , 30.74485 , 24.63548 ,  $\\textbf{16.47052}$\n",
    "\n",
    "Half batch:\n",
    "\n",
    "ADAGRAD: 43.01466 , 25.42512 , 19.26644 , 18.02733\n",
    "\n",
    "ADAM: 30.30286 , 24.31583 , 17.27347 , 21.92849\n",
    "\n",
    "Nesterov: 49.44444 , 24.63725 , 21.94733 , 21.19691\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Suprisingly, full batch outperfomed half batch but this could be due to the limited amount of epochs taken.  Not as suprisingly, more smoothening helped compared to less smoothening and smaller step lengths were better than longer ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
