{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import data_provider as data_provider\n",
    "import config as config\n",
    "import numpy as np\n",
    "from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib.layers import batch_norm, flatten\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore\n",
    "from PIL import Image\n",
    "from tensorpack import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_block = 2\n",
    "init_learning_rate = .1\n",
    "dropout_rate = 0.2\n",
    "nesterov_momentum = 0.9\n",
    "weight_decay = 10\n",
    "\n",
    "exp_decay_steps = 6000\n",
    "exp_decay_rate = 1\n",
    "regularization = .0001\n",
    "\n",
    "#The Number of classes in dataset\n",
    "class_num = 2\n",
    "\n",
    "total_epochs = 2\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_layer(input, filter, kernel, stride=1, layer_name=\"conv\"):\n",
    "    with tf.name_scope(layer_name):\n",
    "        #regularizer controlled here\n",
    "        \n",
    "        network = tf.layers.conv2d(inputs=input, filters=filter, kernel_size=kernel, strides=stride, padding='SAME',kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=regularization))\n",
    "        return network\n",
    "\n",
    "def Global_Average_Pooling(x, stride=1):\n",
    "    \n",
    "\n",
    "    return global_avg_pool(x, name='Global_avg_pooling')\n",
    "\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    with arg_scope([batch_norm],\n",
    "                   scope=scope,\n",
    "                   updates_collections=None,\n",
    "                   decay=0.9,\n",
    "                   center=True,\n",
    "                   scale=True,\n",
    "                   zero_debias_moving_mean=True) :\n",
    "        return tf.cond(training,\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
    "\n",
    "def Drop_out(x, rate, training) :\n",
    "    return tf.layers.dropout(inputs=x, rate=rate, training=training)\n",
    "\n",
    "def Relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def Average_pooling(x, pool_size=[2,2], stride=2, padding='SAME'):\n",
    "    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n",
    "\n",
    "\n",
    "def Max_Pooling(x, pool_sizes, strides, padding='SAME'):\n",
    "    return tf.layers.max_pooling2d(inputs=x, pool_size=pool_sizes, strides=strides, padding=padding)\n",
    "\n",
    "def Concatenation(layers) :\n",
    "    return tf.concat(layers, axis=3)\n",
    "\n",
    "def Linear(x,name) :\n",
    "    return tf.layers.dense(inputs=x, units=class_num, name=name)\n",
    "\n",
    "\n",
    "\n",
    "class DenseNet():\n",
    "    def __init__(self, x, filters, training):\n",
    "        self.filters = filters\n",
    "        self.training = training\n",
    "        self.model = self.Dense_net(x)\n",
    "\n",
    "\n",
    "    def bottleneck_layer(self, i, scope):\n",
    "        # print(x)\n",
    "        with tf.name_scope(scope):\n",
    "\n",
    "\n",
    "            x = Batch_Normalization(i, training=self.training, scope=scope+'_batch2')\n",
    "            x = Relu(x)\n",
    "            x = conv_layer(x, filter=self.filters, kernel=[3,3], layer_name=scope+'_conv2')\n",
    "            x = Drop_out(x, rate=dropout_rate, training=self.training)\n",
    "            i = tf.concat([x,i],3)\n",
    "\n",
    "            # print(x)\n",
    "\n",
    "            return i\n",
    "\n",
    "    def transition_layer(self, x, scope):\n",
    "        shape = x.get_shape().as_list()\n",
    "        in_channel = shape[3]\n",
    "        with tf.name_scope(scope):\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "            x = Relu(x)\n",
    "            x = conv_layer(x, in_channel, kernel=[1,1], layer_name=scope+'_conv1')\n",
    "            x = Drop_out(x, rate=dropout_rate, training=self.training)\n",
    "            x = Average_pooling(x, pool_size=[2,2], stride=2)\n",
    "\n",
    "            return x\n",
    "    \n",
    "    def Dense_net(self, input_x):\n",
    "        \n",
    "        x = conv_layer(input_x, 16, kernel=3)\n",
    "        x = Max_Pooling(x, pool_sizes=2, strides = 2)\n",
    "\n",
    "\n",
    "\n",
    "        #for i in range(self.nb_blocks) :\n",
    "        # 6 -> 12 -> 18\n",
    "        with tf.name_scope('block'+str(1)):\n",
    "            for i in range(5):\n",
    "            \n",
    "                x = self.bottleneck_layer(x, scope='block1' + '_bottleN_' + str(i + 1))\n",
    "                \n",
    "            x = self.transition_layer(x, scope='trans_'+str(1))\n",
    "        with tf.name_scope('block'+str(2)):\n",
    "            for i in range(5):\n",
    "                x = self.bottleneck_layer(x, scope='block2' + '_bottleN_' + str(i + 1))\n",
    "\n",
    "            x = self.transition_layer(x, scope='trans_'+str(2))\n",
    "\n",
    "        with tf.name_scope('block'+str(3)):\n",
    "            \n",
    "            \n",
    "            for i in range(5):\n",
    "                \n",
    "                x = self.bottleneck_layer(x, scope='block3' + '_bottleN_' + str(i + 1))\n",
    "\n",
    "            \n",
    "\n",
    "        #x = self.dense_block(input_x=x, nb_layers=3, layer_name='block'+str(3))\n",
    "        #x = self.transition_layer(x, scope='trans_'+str(i))\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.dense_block(input_x=x, nb_layers=6, layer_name='dense_1')\n",
    "        x = self.transition_layer(x, scope='trans_1')\n",
    "\n",
    "        x = self.dense_block(input_x=x, nb_layers=12, layer_name='dense_2')\n",
    "        x = self.transition_layer(x, scope='trans_2')\n",
    "\n",
    "        x = self.dense_block(input_x=x, nb_layers=48, layer_name='dense_3')\n",
    "        x = self.transition_layer(x, scope='trans_3')\n",
    "        \"\"\"\n",
    "\n",
    "        #x = self.dense_block(input_x=x, nb_layers=4, layer_name='dense_final')\n",
    "\n",
    "        # 20 Layer\n",
    "        x = Batch_Normalization(x, training=self.training, scope='linear_batch')\n",
    "        x = Relu(x)\n",
    "        x = Global_Average_Pooling(x)\n",
    "        x = flatten(x)\n",
    "        x = Linear(x,'linear0')\n",
    "        return x\n",
    "    \n",
    "def densenet_train():\n",
    "    image_batch_placeholder = tf.placeholder(\"float32\", shape=[None, 256, 256, 3])\n",
    "    label_batch_placeholder = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "    #   if_training_placeholder = tf.placeholder(tf.bool, shape=[])\n",
    "\n",
    "    image_batch, label_batch = data_provider.feed_data(if_random = True, if_training = True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    training_flag = tf.placeholder(tf.bool)\n",
    "\n",
    "    logits = DenseNet(x = image_batch_placeholder, filters = 12, training=training_flag).model\n",
    "\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=label_batch_placeholder, logits=logits))\n",
    "    #wd_decay = tf.multiply(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_decay')\n",
    "    #add_moving_summary(loss,wd_decay)\n",
    "    logits_batch = tf.to_int64(tf.arg_max(logits, dimension = 1))\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label_batch_placeholder, 1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n",
    "    #loss = tf.losses.mean_squared_error(labels=label_batch_placeholder, predictions=logits)\n",
    "\n",
    "    tf.summary.scalar('loss', loss) # create a summary for training loss\n",
    "\n",
    "    regularzation_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #tf.summary.scalar('regularzation_loss', regularzation_loss)\n",
    "    reg_constant = 0.01\n",
    "    \n",
    "    total_loss = sum(regularzation_loss) + loss\n",
    "    tf.summary.scalar('total_loss', total_loss)\n",
    "\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    #can change to constant here. \n",
    "    learning_rate = tf.train.exponential_decay(learning_rate=init_learning_rate,\n",
    "                                               global_step=global_step,\n",
    "                                               decay_steps= exp_decay_steps,\n",
    "                                               decay_rate= exp_decay_rate,\n",
    "                                               staircase=True)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "    #can change to ADAM here or gradient descent regular \n",
    "    \n",
    "    if(momentum_type == \"Nesterov\"):\n",
    "        train_step = tf.train.MomentumOptimizer(learning_rate = learning_rate, momentum=nesterov_momentum,use_nesterov=True).minimize(total_loss, global_step=global_step)\n",
    "    if(momentum_type == \"ADAM\"):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate=init_learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8,\n",
    "               use_locking=False, name=\"Adam\").minimize(total_loss, global_step=global_step)\n",
    "    if(momentum_type == \"ADAGRAD\"):\n",
    "        train_step = tf.train.AdagradOptimizer(init_learning_rate, initial_accumulator_value=0.1,\n",
    "               use_locking=False, name=\"Adagrad\").minimize(total_loss, global_step=global_step)\n",
    "\n",
    "    summary_op = tf.summary.merge_all()  # merge all summaries into a single \"operation\" which we can execute in a session\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=100)\n",
    "    all_trainable_vars = tf.reduce_sum([tf.reduce_prod(v.shape) for v in tf.trainable_variables()])\n",
    "\n",
    "    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.666)\n",
    "\n",
    "\n",
    "    config = tf.ConfigProto(log_device_placement=False)\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    accuracy_accu = 0\n",
    "\n",
    "    checkpoint = None\n",
    "    #if restoring from checkpoint uncomment this\n",
    "    #checkpoint = tf.train.get_checkpoint_state(\"./brehis_ici_model_binary\")\n",
    "    if(checkpoint != None):\n",
    "        tf.logging.info(\"Restoring full model from checkpoint file %s\",checkpoint.model_checkpoint_path)\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess = sess)\n",
    "\n",
    "    check_points = 20\n",
    "    for epoch in range(0,total_epochs):\n",
    "        print(\"epoch is\" + str(epoch))\n",
    "        for check_point in tqdm(range(check_points),    \n",
    "        bar_format=\"{l_bar}%s{bar}%s{r_bar}\" % (Fore.BLUE, Fore.RESET)):\n",
    "\n",
    "            image_batch_train, label_batch_train = sess.run([image_batch, label_batch])\n",
    "\n",
    "            _, training_loss, _global_step, summary = sess.run([train_step, loss, global_step, summary_op],\n",
    "                                                 feed_dict={image_batch_placeholder: image_batch_train,\n",
    "                                                            label_batch_placeholder: label_batch_train,\n",
    "                                                            training_flag: True})\n",
    "            #bool(check_point != 0)\n",
    "            if(bool(check_point%10 == 0)):\n",
    "                print(_)\n",
    "                print(\"training checkpoint: \", check_point + epoch * check_points)\n",
    "                print(\"training loss: \", training_loss)\n",
    "                print(\"Total weight:\", sess.run(all_trainable_vars))\n",
    "                print(\"Current Learning-Rate:\", sess.run(learning_rate))\n",
    "\n",
    "                summary_writer.add_summary(summary, _global_step)\n",
    "\n",
    "        \n",
    "        #if you want to save the checkpoint uncomment this\n",
    "        #saver.save(sess, \"./brehis_ici_model_binary/ch.ckpt\", _global_step)\n",
    "        if _global_step >= 240000:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tf.reset_default_graph()\n",
    "    densenet_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "INFO:tensorflow:Restoring full model from checkpoint file ./brehis_ici_model_binary\\ch.ckpt-80\n",
      "INFO:tensorflow:Restoring parameters from ./brehis_ici_model_binary\\ch.ckpt-80\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  13.208616\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:57<09:43, 58.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  26.060658\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:37<00:00, 57.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  22.157194\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:33<09:30, 57.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  23.216454\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:02<00:00, 56.51s/it]\n"
     ]
    }
   ],
   "source": [
    "momentum_type = \"ADAM\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "INFO:tensorflow:Restoring full model from checkpoint file ./brehis_ici_model_binary\\ch.ckpt-120\n",
      "INFO:tensorflow:Restoring parameters from ./brehis_ici_model_binary\\ch.ckpt-120\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  20.708124\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:53<09:47, 58.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  48.04075\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:30<00:00, 57.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  134.737\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:39<09:35, 57.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  76.29205\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:17<00:00, 57.86s/it]\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = 1.0\n",
    "momentum_type = \"ADAM\"\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  38.1938\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [14:14<12:19, 73.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  15.957069\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [25:18<00:00, 66.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  21.710157\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [11:05<11:04, 66.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  26.298061\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [22:17<00:00, 65.10s/it]\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = .001\n",
    "momentum_type = \"Nesterov\"\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  50.262753\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:59<09:48, 58.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  24.891222\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:46<00:00, 59.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  19.50465\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:41<09:41, 58.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  20.665682\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:30<00:00, 58.25s/it]\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = .001\n",
    "regularization = .001\n",
    "momentum_type = \"ADAM\"\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  33.01615\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:59<09:49, 58.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  22.585104\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:43<00:00, 57.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  10.085077\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:43<09:41, 58.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  27.59055\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:29<00:00, 58.91s/it]\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = .001\n",
    "regularization = .001\n",
    "momentum_type = \"Nesterov\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  38.40235\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:18<10:01, 60.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  24.261717\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:18<00:00, 60.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  24.520802\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:46<09:47, 58.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  22.042383\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:33<00:00, 58.57s/it]\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = .001\n",
    "regularization = .001\n",
    "momentum_type = \"ADAGRAD\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  58.831734\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:09<10:05, 60.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  19.950806\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:09<00:00, 59.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  23.764097\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:00<09:58, 59.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  17.118456\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:56<00:00, 59.17s/it]\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = .001\n",
    "regularization = .01\n",
    "momentum_type = \"ADAM\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  33.285606\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:04<09:58, 59.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  30.744848\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:07<00:00, 60.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  24.635479\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:06<10:07, 60.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  16.470516\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:04<00:00, 60.37s/it]\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = .001\n",
    "regularization = .01\n",
    "momentum_type = \"Nesterov\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  32.166283\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:13<10:10, 61.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  21.327787\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:17<00:00, 60.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  18.086004\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:57<09:59, 59.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  15.82132\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:49<00:00, 58.50s/it]\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = .001\n",
    "regularization = .01\n",
    "momentum_type = \"ADAGRAD\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  45.232258\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:11<10:02, 60.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  13.656053\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:13<00:00, 59.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  22.187944\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:46<09:47, 58.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  15.338309\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:43<00:00, 60.09s/it]\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = .001\n",
    "regularization = .1\n",
    "momentum_type = \"ADAM\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  37.52027\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:52<09:46, 58.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  20.298044\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:40<00:00, 59.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  24.766613\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:57<09:56, 59.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  29.773773\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:25<00:00, 57.88s/it]\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = .001\n",
    "regularization = .1\n",
    "momentum_type = \"Nesterov\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  35.024323\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:09<09:54, 59.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  21.662045\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:09<00:00, 59.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  25.997416\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:47<09:40, 58.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  18.493244\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:42<00:00, 59.28s/it]\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = .001\n",
    "regularization = .1\n",
    "momentum_type = \"ADAGRAD\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  43.014656\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:21<10:14, 61.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  25.425123\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:27<00:00, 60.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  19.266441\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:00<10:01, 60.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  18.027332\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:02<00:00, 60.82s/it]\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = .001\n",
    "regularization = .01\n",
    "momentum_type = \"ADAGRAD\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  49.444435\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:11<10:02, 60.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  24.637245\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:05<00:00, 59.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  21.94733\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:01<10:01, 60.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  21.196907\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:54<00:00, 59.49s/it]\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = .001\n",
    "regularization = .01\n",
    "momentum_type = \"Nesterov\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 9600 images before starting to train. This will take some time.\n",
      "epoch is0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  0\n",
      "training loss:  30.302864\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:23<10:09, 60.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  10\n",
      "training loss:  24.315826\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:21<00:00, 59.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  20\n",
      "training loss:  17.273472\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:15<10:41, 64.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "training checkpoint:  30\n",
      "training loss:  21.928492\n",
      "Total weight: 191322\n",
      "Current Learning-Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [21:08<00:00, 64.99s/it]\n"
     ]
    }
   ],
   "source": [
    "init_learning_rate = .001\n",
    "regularization = .1\n",
    "momentum_type = \"ADAM\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
